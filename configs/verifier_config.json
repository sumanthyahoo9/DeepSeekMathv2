# Verifier Training Configuration
# Configuration for training proof verification models using GRPO

# Model Configuration
model:
  name: "deepseek-math-7b-base"  # Base model to fine-tune
  # For production, use: "deepseek-v3.2-exp-base"
  checkpoint: null  # Path to checkpoint to resume from (null = train from scratch)
  max_length: 8192  # Maximum sequence length
  
# Dataset Configuration
data:
  train_dataset: "data/verification/train.jsonl"  # Training data path
  val_dataset: "data/verification/val.jsonl"      # Validation data path
  # Expected format: {"problem": "...", "proof": "...", "score": 0/0.5/1}
  
  # Data preprocessing
  max_problems: null  # Limit number of problems (null = use all)
  shuffle_seed: 42
  
# GRPO Configuration (from paper)
grpo:
  group_size: 4  # K samples per problem (paper uses 4)
  kl_coef: 0.01  # KL divergence penalty coefficient
  clip_range: 0.2  # PPO-style clipping range
  
# Reward Configuration (from paper Section 2.1.1)
reward:
  alpha_format: 0.125  # Weight for format reward (R_format)
  alpha_score: 0.875   # Weight for score reward (R_score)
  
  # Meta-verification (optional, from Section 2.1.2)
  use_meta_verification: false  # Enable meta-verification
  meta_verifier_checkpoint: null  # Path to meta-verifier model
  
# Training Configuration
training:
  # Optimization
  learning_rate: 1.0e-6  # Learning rate
  batch_size: 8  # Number of problems per batch
  gradient_accumulation_steps: 4  # Accumulate gradients over N steps
  num_epochs: 3  # Number of training epochs
  max_grad_norm: 1.0  # Gradient clipping threshold
  weight_decay: 0.1  # Weight decay for AdamW
  
  # Learning rate schedule
  warmup_steps: 100  # Number of warmup steps
  scheduler_type: "cosine"  # cosine or linear
  min_lr_ratio: 0.1  # Minimum LR as ratio of initial LR
  
  # Mixed precision
  fp16: false  # Use FP16 training (for older GPUs)
  bf16: true   # Use BF16 training (recommended for modern GPUs)
  
# Generation Configuration
generation:
  max_new_tokens: 2048  # Maximum tokens to generate for verification
  temperature: 0.8  # Sampling temperature
  top_p: 0.95  # Nucleus sampling threshold
  top_k: 50  # Top-k sampling
  do_sample: true  # Enable sampling (false = greedy)
  num_beams: 1  # Beam search (1 = no beam search)
  
# Logging Configuration
logging:
  output_dir: "checkpoints/verifier"  # Directory for checkpoints and logs
  logging_steps: 10  # Log metrics every N steps
  eval_steps: 500  # Evaluate every N steps
  save_steps: 1000  # Save checkpoint every N steps
  
  # Metrics to track
  track_metrics:
    - loss
    - reward
    - policy_loss
    - kl_div
    - format_accuracy
    - score_accuracy
  
  # Weights & Biases (optional)
  use_wandb: false  # Enable W&B logging
  wandb_project: "deepseek-math-v2"
  wandb_run_name: "verifier-training"
  
# Checkpointing Configuration
checkpointing:
  max_checkpoints: 3  # Keep only N best checkpoints
  metric_for_best: "mean_reward"  # Metric to determine best checkpoint
  save_optimizer_state: true  # Save optimizer state in checkpoints
  save_total_limit: 5  # Total checkpoint limit (includes best + latest)
  
# Evaluation Configuration
evaluation:
  eval_batch_size: 16  # Batch size for evaluation
  eval_max_samples: null  # Max samples to evaluate (null = all)
  
  # Metrics to compute during evaluation
  compute_metrics:
    - accuracy  # Score prediction accuracy
    - f1_score  # F1 for binary correct/incorrect
    - mae  # Mean absolute error for scores
    - confusion_matrix  # Confusion matrix for 3 classes
  
# Hardware Configuration
hardware:
  device: "cuda"  # cuda or cpu
  gpu_ids: [0]  # GPU IDs to use (multi-GPU: [0, 1, 2, 3])
  num_workers: 4  # Number of dataloader workers
  pin_memory: true  # Pin memory for faster data transfer
  
  # DeepSpeed (optional, for large models)
  use_deepspeed: false  # Enable DeepSpeed
  deepspeed_config: "configs/deepspeed_config.json"
  
# Reproducibility
seed: 42  # Random seed for reproducibility

# Advanced Options
advanced:
  # Gradient checkpointing (saves memory at cost of speed)
  gradient_checkpointing: false
  
  # Compilation (PyTorch 2.0+)
  torch_compile: false  # Enable torch.compile()
  
  # Debugging
  debug_mode: false  # Enable debug logging
  profile_training: false  # Profile training performance
  
  # Early stopping
  early_stopping_patience: 5  # Stop if no improvement after N evals
  early_stopping_threshold: 0.001  # Minimum improvement threshold