# DeepSeekMath-V2 Implementation

[![Tests](https://img.shields.io/badge/tests-131%20passing-brightgreen)]()
[![Python](https://img.shields.io/badge/python-3.10%2B-blue)]()
[![License](https://img.shields.io/badge/license-MIT-blue)]()

A clean, modular implementation of [DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning](https://github.com/deepseek-ai/DeepSeek-Math-V2) for learning distributed training, reinforcement learning (GRPO), and mathematical reasoning systems.

---

## ğŸ¯ Project Goals

This implementation focuses on **learning and understanding** the architecture and training methodology of DeepSeekMath-V2:

1. **Understand the three-model system** (Verifier, Meta-Verifier, Generator)
2. **Learn distributed training** (DeepSpeed, Megatron-LM patterns)
3. **Implement GRPO** (Group Relative Policy Optimization)
4. **Explore self-verification** mechanisms in LLMs
5. **Practice ML engineering** best practices (modular code, testing, documentation)

**Note:** This is a **learning implementation** with a limited GPU budget (1x T4, 16GB). For production-scale training, see the official DeepSeek repository.

---

## ğŸ“Š What is DeepSeekMath-V2?

DeepSeekMath-V2 is a system for training LLMs to solve mathematical proofs with **self-verification**:

### **The Three-Model System:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Verifier   â”‚ â† Scores proofs (0, 0.5, 1)
â”‚    (Ï€_Ï†)    â”‚   Identifies errors
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Meta-Verifierâ”‚ â† Checks verifier quality
â”‚    (Ï€_Î·)    â”‚   Prevents hallucinations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generator  â”‚ â† Generates proofs
â”‚    (Ï€_Î¸)    â”‚   Self-verifies & refines
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Key Innovation:**
Models learn to **verify their own reasoning** and iteratively improve proofs until no issues remain.

### **Results (from paper):**
- ğŸ¥‡ Gold medal: IMO 2025, CMO 2024
- ğŸ“ˆ 118/120 on Putnam 2024 (vs. human max of 90)

---

## ğŸš€ Quick Start

### **1. Setup Environment**

```bash
# Clone repository
git clone https://github.com/your-username/deepseek-math-v2
cd deepseek-math-v2

# Run automated setup
chmod +x scripts/00_setup_environment.sh
bash scripts/00_setup_environment.sh

# Activate virtual environment
source venv/bin/activate
```

### **2. Run Tests**

```bash
# Run all tests (131 tests)
pytest tests/ -v

# Expected output: 131 passed
```

### **3. Try the Modules**

```python
# Example: Generate a prompt
from src.utils.prompts import get_proof_generation_prompt

problem = "Prove that sqrt(2) is irrational"
prompt = get_proof_generation_prompt(problem)
print(prompt)

# Example: Load configuration
from src.utils.config_loader import create_default_config

config = create_default_config("my_experiment")
print(f"Learning rate: {config.training.learning_rate}")

# Example: Compute rewards
from src.training.reward_functions import compute_score_reward

reward = compute_score_reward(predicted=0.5, ground_truth=1.0)
print(f"Reward: {reward}")  # 0.5
```

---

## ğŸ“ Repository Structure

```
deepseek-math-v2/
â”œâ”€â”€ README.md                    â† You are here
â”œâ”€â”€ requirements.txt             â† Dependencies
â”œâ”€â”€ setup.py                     â† Package setup
â”‚
â”œâ”€â”€ configs/                     â† YAML configurations
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ data/
â”‚
â”œâ”€â”€ src/                         â† Source code (7 modules, ~1,940 lines)
â”‚   â”œâ”€â”€ utils/                   â† Utilities
â”‚   â”‚   â”œâ”€â”€ prompts.py          â† Prompt templates
â”‚   â”‚   â”œâ”€â”€ config_loader.py    â† Config management
â”‚   â”‚   â””â”€â”€ profiling.py        â† GPU profiling (TODO)
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                    â† Data pipeline
â”‚   â”‚   â”œâ”€â”€ proof_dataset.py    â† PyTorch Datasets
â”‚   â”‚   â””â”€â”€ data_collator.py    â† Batch preparation
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                   â† Model wrappers
â”‚   â”‚   â”œâ”€â”€ base_model.py       â† Base model utilities
â”‚   â”‚   â”œâ”€â”€ model_utils.py      â† Helper functions
â”‚   â”‚   â”œâ”€â”€ verifier.py         â† Verifier (TODO)
â”‚   â”‚   â””â”€â”€ generator.py        â† Generator (TODO)
â”‚   â”‚
â”‚   â””â”€â”€ training/                â† Training logic
â”‚       â”œâ”€â”€ reward_functions.py â† GRPO rewards
â”‚       â”œâ”€â”€ grpo_trainer.py     â† GRPO implementation (TODO)
â”‚       â””â”€â”€ verifier_trainer.py â† Training loops (TODO)
â”‚
â”œâ”€â”€ scripts/                     â† Executable scripts
â”‚   â”œâ”€â”€ 00_setup_environment.sh  â† Environment setup
â”‚   â”œâ”€â”€ 01_upgrade_to_gpu.sh     â† GPU upgrade
â”‚   â”œâ”€â”€ 10_train_verifier.py     â† Training scripts (TODO)
â”‚   â”œâ”€â”€ 20_auto_label_proofs.py  â† Auto-labeling (TODO)
â”‚   â””â”€â”€ 30_evaluate.py           â† Evaluation (TODO)
â”‚
â”œâ”€â”€ tests/                       â† Unit tests (131 passing!)
â”‚   â”œâ”€â”€ test_prompts.py
â”‚   â”œâ”€â”€ test_config_loader.py
â”‚   â”œâ”€â”€ test_reward_functions.py
â”‚   â”œâ”€â”€ test_proof_dataset.py
â”‚   â”œâ”€â”€ test_data_collator.py
â”‚   â”œâ”€â”€ test_base_model.py
â”‚   â””â”€â”€ test_model_utils.py
â”‚
â”œâ”€â”€ notebooks/                   â† Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_architecture.ipynb
â”‚   â””â”€â”€ 03_grpo_analysis.ipynb
â”‚
â”œâ”€â”€ experiments/                 â† Experiment outputs
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ results/
â”‚
â””â”€â”€ docs/                        â† Documentation
    â”œâ”€â”€ setup_guide.md
    â”œâ”€â”€ BASE_MODEL_EXPLAINED.md
    â”œâ”€â”€ DATA_COLLATOR_EXPLAINED.md
    â””â”€â”€ MODEL_UTILS_EXPLAINED.md
```

---

## ğŸ§© Implementation Progress

### âœ… **Completed Modules** (7/12)

| Module | Lines | Tests | Status |
|--------|-------|-------|--------|
| `prompts.py` | 280 | 14 âœ“ | âœ… Complete |
| `config_loader.py` | 240 | 9 âœ“ | âœ… Complete |
| `reward_functions.py` | 220 | 16 âœ“ | âœ… Complete |
| `proof_dataset.py` | 250 | 18 âœ“ | âœ… Complete |
| `data_collator.py` | 280 | 20 âœ“ | âœ… Complete |
| `base_model.py` | 290 | 20 âœ“ | âœ… Complete |
| `model_utils.py` | 380 | 34 âœ“ | âœ… Complete |
| **Total** | **1,940** | **131** | **53% done** |

### â­ï¸ **Next Steps**

- [ ] `verifier.py` - Proof verification model
- [ ] `generator.py` - Proof generation model
- [ ] `grpo_trainer.py` - GRPO training loop
- [ ] `verifier_trainer.py` - Verifier training
- [ ] `auto_labeling.py` - Automated proof labeling

---

## ğŸ“ Key Concepts

### **1. Three-Model Training Pipeline**

```
Phase 1: Train Verifier
  â”œâ”€ Input: (problem, proof, expert_score)
  â”œâ”€ Output: Analysis + Score
  â””â”€ Reward: R_format Ã— R_score

Phase 2: Train Meta-Verifier
  â”œâ”€ Input: (problem, proof, verifier_analysis, meta_score)
  â”œâ”€ Output: Quality assessment
  â””â”€ Reward: R_format Ã— R_score Ã— R_meta

Phase 3: Enhanced Verifier
  â”œâ”€ Use meta-verifier feedback
  â””â”€ Reduce hallucinated issues

Phase 4: Train Generator
  â”œâ”€ Input: problem
  â”œâ”€ Output: proof + self-analysis
  â””â”€ Reward: Î±Â·R_Y + Î²Â·R_Z (Î±=0.76, Î²=0.24)

Phase 5: Auto-Label & Iterate
  â”œâ”€ Generate hard proofs
  â”œâ”€ Scale verification (n=64 samples)
  â”œâ”€ Auto-label via majority voting
  â””â”€ Retrain verifier â†’ Loop
```

### **2. GRPO (Group Relative Policy Optimization)**

RL algorithm that rewards based on **relative ranking** within a group, not absolute scores. More stable than PPO for mathematical reasoning.

### **3. Self-Verification**

Model generates: `Proof + Self-Analysis + Self-Score`

Incentivized to:
- Identify issues in own work
- Fix issues before finalizing
- Accurately assess proof quality

---

## ğŸ”§ Technical Details

### **Model Architecture**
- **Base:** DeepSeek-V3.2-Exp-Base (MoE, ~236B total params, ~37B active)
- **Context:** 128K tokens
- **Precision:** bfloat16
- **Hardware (paper):** Multi-GPU cluster with DeepSpeed ZeRO-3

### **Our Setup (Learning)**
- **GPU:** 1x NVIDIA T4 (16GB) - **cannot fit full model!**
- **Alternative:** Use DeepSeek-Math-7B or LoRA fine-tuning
- **Budget:** Â£204 (~$257) for ~500 hours of compute

### **Key Technologies**
- **PyTorch** - Deep learning framework
- **Transformers** - HuggingFace model loading
- **DeepSpeed** - Distributed training (when GPU available)
- **Pydantic** - Type-safe configuration
- **Pytest** - Testing framework

---

## ğŸ“š Documentation

### **Getting Started**
- [Setup Guide](docs/setup_guide.md) - Environment setup
- [Setup Explanation](docs/setup_explanation.md) - How setup scripts work (interview prep)

### **Module Explanations**
- [BASE_MODEL_EXPLAINED.md](docs/BASE_MODEL_EXPLAINED.md) - Model loading/saving
- [DATA_COLLATOR_EXPLAINED.md](docs/DATA_COLLATOR_EXPLAINED.md) - Batch preparation
- [MODEL_UTILS_EXPLAINED.md](docs/MODEL_UTILS_EXPLAINED.md) - Helper utilities

### **Training**
- [PHASE1_SUMMARY.md](docs/PHASE1_SUMMARY.md) - Initial implementation summary

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_prompts.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run fast tests only
pytest tests/ -m "not slow"
```

**Current Coverage:** 131 tests, all passing âœ…

---

## ğŸ’¡ Design Philosophy

### **1. Modular Architecture**
- Each file ~200-300 lines (not 1000+!)
- Single responsibility per module
- Easy to understand and modify

### **2. Test-Driven Development**
- Tests written alongside code
- 100% of implemented functions tested
- Can develop on CPU, no GPU needed

### **3. Learning-Focused**
- Clear documentation and comments
- Interview prep materials included
- Explanations for "why" not just "what"

### **4. Production Patterns**
- Type hints everywhere
- Pydantic validation
- Proper error handling
- Clean separation of concerns

---

## ğŸš§ Limitations & Future Work

### **Current Limitations**
- âŒ Full model doesn't fit on single T4 GPU
- âŒ Training not yet implemented (GRPO trainer in progress)
- âŒ No actual proof scraping from AoPS
- âŒ Mock mode for testing (no real model loading)

### **Planned Improvements**
- [ ] LoRA/QLoRA support for T4 training
- [ ] Implement full GRPO training loop
- [ ] Add AoPS data scraper
- [ ] Real-time training monitoring
- [ ] GPU profiling with PyCUDA
- [ ] Multi-GPU support with DeepSpeed

---

## ğŸ“– References

### **Papers**
- [DeepSeekMath-V2 Paper](https://github.com/deepseek-ai/DeepSeek-Math-V2) - Original paper
- [GRPO Paper](https://arxiv.org/abs/2402.03300) - Group Relative Policy Optimization

### **Related Work**
- [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) - Base model family
- [AlphaProof](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) - Formal theorem proving

---

## ğŸ¤ Contributing

This is a **learning project**, but contributions welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`pytest tests/`)
5. Commit (`git commit -m 'Add amazing feature'`)
6. Push (`git push origin feature/amazing-feature`)
7. Open a Pull Request

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **DeepSeek AI** - Original paper and implementation
- **HuggingFace** - Transformers library
- **PyTorch Team** - Deep learning framework

---

## ğŸ“Š Project Stats

- **Language:** Python 3.11+
- **Lines of Code:** ~1,940 (source) + ~1,730 (tests)
- **Tests:** 131 passing
- **Modules:** 7 complete, 5 in progress
- **Documentation:** 5 detailed guides
- **License:** MIT